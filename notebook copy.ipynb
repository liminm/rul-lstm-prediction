{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543ff77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f2cc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"data\")\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "FD_TAGS = [\"FD001\", \"FD003\"]\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc63c22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_path = DATA_DIR / \"CMAPSSData.zip\"\n",
    "\n",
    "if not zip_path.exists():\n",
    "    os.system(f\"wget https://data.nasa.gov/docs/legacy/CMAPSSData.zip -O {zip_path}\")\n",
    "\n",
    "os.system(f\"unzip -o {zip_path} -d {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c448285",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_names = [\"unit_nr\", \"time_cycles\"]\n",
    "setting_names = [\"setting_1\", \"setting_2\", \"setting_3\"]\n",
    "sensor_names = [f\"s_{i}\" for i in range(1, 22)]\n",
    "col_names = index_names + setting_names + sensor_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb82ce84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fd(fd_tag: str, data_dir: Path) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    train_path = data_dir / f\"train_{fd_tag}.txt\"\n",
    "    test_path = data_dir / f\"test_{fd_tag}.txt\"\n",
    "    rul_path = data_dir / f\"RUL_{fd_tag}.txt\"\n",
    "\n",
    "    train_df = pd.read_csv(train_path, sep=r\"\\s+\", header=None, names=col_names)\n",
    "    test_df = pd.read_csv(test_path, sep=r\"\\s+\", header=None, names=col_names)\n",
    "    rul_df = pd.read_csv(rul_path, header=None, names=[\"RUL_truth\"])\n",
    "\n",
    "    max_cycle = train_df.groupby(\"unit_nr\")[\"time_cycles\"].max().rename(\"max_cycle\")\n",
    "    train_df = train_df.merge(max_cycle, left_on=\"unit_nr\", right_index=True)\n",
    "    train_df[\"RUL\"] = train_df[\"max_cycle\"] - train_df[\"time_cycles\"]\n",
    "\n",
    "    train_df[\"fd\"] = fd_tag\n",
    "    test_df[\"fd\"] = fd_tag\n",
    "    rul_df[\"fd\"] = fd_tag\n",
    "\n",
    "    return train_df, test_df, rul_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdaf66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_global_unit_ids(train_df: pd.DataFrame, test_df: pd.DataFrame, fd_tag: str, next_unit: int):\n",
    "    uniq_units_train = sorted(train_df[\"unit_nr\"].unique())\n",
    "    mapping_train = {u: next_unit + i for i, u in enumerate(uniq_units_train)}\n",
    "\n",
    "    train_df = train_df.assign(\n",
    "        unit_nr_orig=train_df[\"unit_nr\"],\n",
    "        unit_nr=train_df[\"unit_nr\"].map(mapping_train),\n",
    "    )\n",
    "\n",
    "    test_df = test_df.assign(\n",
    "        unit_nr_orig=test_df[\"unit_nr\"],\n",
    "        unit_nr=test_df[\"unit_nr\"] + next_unit - 1,\n",
    "    )\n",
    "\n",
    "    next_unit = next_unit + len(uniq_units_train)\n",
    "    return train_df, test_df, next_unit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc090e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a directory for the data\n",
    "if not os.path.exists('data2'):\n",
    "    os.makedirs('data2')\n",
    "\n",
    "# 2. Download the zip file (using a reliable mirror for the NASA dataset)\n",
    "!wget https://data.nasa.gov/docs/legacy/CMAPSSData.zip -O data2/CMAPSSData.zip\n",
    "\n",
    "# 3. Unzip it\n",
    "!unzip -o data2/CMAPSSData.zip -d data2/\n",
    "\n",
    "print(\"Data downloaded and extracted!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bef4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset has 26 columns\n",
    "# 1. Unit Number (Which engine is it?)\n",
    "# 2. Time Cycles (How long has it been running?)\n",
    "# 3-5. Operational Settings (Altitude, Speed, etc.)\n",
    "# 6-26. Sensor Readings (s1 to s21)\n",
    "\n",
    "index_names = ['unit_nr', 'time_cycles']\n",
    "setting_names = ['setting_1', 'setting_2', 'setting_3']\n",
    "sensor_names = ['s_{}'.format(i) for i in range(1, 22)] \n",
    "col_names = index_names + setting_names + sensor_names\n",
    "\n",
    "print(col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898a95c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset has 26 columns\n",
    "# 1. Unit Number (Which engine is it?)\n",
    "# 2. Time Cycles (How long has it been running?)\n",
    "# 3-5. Operational Settings (Altitude, Speed, etc.)\n",
    "# 6-26. Sensor Readings (s1 to s21)\n",
    "\n",
    "index_names = ['unit_nr', 'time_cycles']\n",
    "setting_names = ['setting_1', 'setting_2', 'setting_3']\n",
    "sensor_names = ['s_{}'.format(i) for i in range(1, 22)] \n",
    "col_names = index_names + setting_names + sensor_names\n",
    "\n",
    "print(col_names)\n",
    "\n",
    "def load_fd(fd_tag):\n",
    "    train_path = f\"data/train_{fd_tag}.txt\"\n",
    "    test_path  = f\"data/test_{fd_tag}.txt\"\n",
    "    rul_path   = f\"data/RUL_{fd_tag}.txt\"\n",
    "\n",
    "    raw_train_df = pd.read_csv(train_path, sep=r'\\s+', header=None, names=col_names)\n",
    "    raw_test_df  = pd.read_csv(test_path,  sep=r'\\s+', header=None, names=col_names)\n",
    "    raw_rul_labels_df = pd.read_csv(rul_path, header=None, names=['RUL_truth'])\n",
    "\n",
    "    # train labels: compute RUL from run-to-failure\n",
    "    max_cycle = raw_train_df.groupby('unit_nr')['time_cycles'].max().rename('max_cycle')\n",
    "    raw_train_df = raw_train_df.merge(max_cycle, left_on='unit_nr', right_index=True)\n",
    "    raw_train_df['RUL'] = raw_train_df['max_cycle'] - raw_train_df['time_cycles']\n",
    "\n",
    "    # test labels: provided separately\n",
    "    return raw_train_df, raw_test_df, raw_rul_labels_df\n",
    "\n",
    "next_unit = 1\n",
    "train_dfs = []\n",
    "test_dfs = []\n",
    "test_rul_labels = []\n",
    "\n",
    "data_tags = [\"FD001\"]\n",
    "\n",
    "for fd_tag in data_tags:\n",
    "    train_df_chunk, test_df_chunk, rul_labels_chunk = load_fd(fd_tag)\n",
    "    train_df_chunk['fd'] = fd_tag\n",
    "    test_df_chunk['fd'] = fd_tag\n",
    "    test_df_chunk = test_df_chunk.assign(\n",
    "        unit_nr_orig=test_df_chunk['unit_nr'],\n",
    "        unit_nr=test_df_chunk['unit_nr'] + next_unit - 1\n",
    "    )\n",
    "    test_dfs.append(test_df_chunk)\n",
    "    test_rul_labels.append(rul_labels_chunk)\n",
    "\n",
    "    # make a mapping for this FD's units\n",
    "    uniq_units = sorted(train_df_chunk['unit_nr'].unique())\n",
    "    mapping = {u: next_unit + i for i, u in enumerate(uniq_units)}\n",
    "    next_unit += len(uniq_units)\n",
    "\n",
    "    train_df_chunk = train_df_chunk.assign(\n",
    "        unit_nr_orig=train_df_chunk['unit_nr'],\n",
    "        unit_nr=train_df_chunk['unit_nr'].map(mapping),\n",
    "        fd=fd_tag\n",
    "    )\n",
    "    train_dfs.append(train_df_chunk)\n",
    "    \n",
    "    \n",
    "train_df = pd.concat(train_dfs, ignore_index=True)\n",
    "test_df = pd.concat(test_dfs, ignore_index=True)\n",
    "rul_labels_df = pd.concat(test_rul_labels, ignore_index=True)\n",
    "\n",
    "print(f\"Train data shape: {train_df.shape}\")\n",
    "print(f\"Test data shape: {test_df.shape}\")\n",
    "print(f\"RUL labels shape: {rul_labels_df.shape}\")\n",
    "\n",
    "engine_ids = train_df['unit_nr'].unique()\n",
    "\n",
    "train_ids = engine_ids[:80]\n",
    "val_ids = engine_ids[80:]\n",
    "print(\"len train ids:\", len(train_ids))\n",
    "print(\"len val ids:\", len(val_ids))\n",
    "print(\"len engine ids:\", len(engine_ids))\n",
    "\n",
    "train_split_df = train_df[train_df['unit_nr'].isin(train_ids)]\n",
    "val_split_df = train_df[train_df['unit_nr'].isin(val_ids)]\n",
    "print(\"Train df shape:\", train_split_df.shape)\n",
    "print(\"Val df shape:\", val_split_df.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084b5cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_unit = 1\n",
    "train_parts = []\n",
    "test_parts = []\n",
    "rul_parts = []\n",
    "\n",
    "for fd_tag in FD_TAGS:\n",
    "    tr, te, rul = load_fd(fd_tag, DATA_DIR)\n",
    "\n",
    "    tr, te, next_unit = make_global_unit_ids(tr, te, fd_tag, next_unit)\n",
    "\n",
    "    train_parts.append(tr)\n",
    "    test_parts.append(te)\n",
    "    rul_parts.append(rul)\n",
    "\n",
    "train_df = pd.concat(train_parts, ignore_index=True)\n",
    "test_df = pd.concat(test_parts, ignore_index=True)\n",
    "rul_labels_df = pd.concat(rul_parts, ignore_index=True)\n",
    "\n",
    "train_df = train_df.sort_values([\"unit_nr\", \"time_cycles\"]).reset_index(drop=True)\n",
    "test_df = test_df.sort_values([\"unit_nr\", \"time_cycles\"]).reset_index(drop=True)\n",
    "\n",
    "print(\"Train shape:\", train_df.shape)\n",
    "print(\"Test shape:\", test_df.shape)\n",
    "print(\"RUL labels shape:\", rul_labels_df.shape)\n",
    "print(\"Unique train units:\", train_df[\"unit_nr\"].nunique())\n",
    "print(\"Unique test units:\", test_df[\"unit_nr\"].nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e011cc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_unit(df: pd.DataFrame, val_frac: float, seed: int):\n",
    "    units = df[\"unit_nr\"].unique()\n",
    "    rng = np.random.default_rng(seed)\n",
    "    rng.shuffle(units)\n",
    "\n",
    "    n_val = int(np.ceil(len(units) * val_frac))\n",
    "    val_units = set(units[:n_val])\n",
    "    train_units = set(units[n_val:])\n",
    "\n",
    "    train_split = df[df[\"unit_nr\"].isin(train_units)].copy()\n",
    "    val_split = df[df[\"unit_nr\"].isin(val_units)].copy()\n",
    "\n",
    "    return train_split, val_split, sorted(train_units), sorted(val_units)\n",
    "\n",
    "train_split_df, val_split_df, train_units, val_units = split_by_unit(train_df, val_frac=0.2, seed=SEED)\n",
    "\n",
    "print(\"Train split shape:\", train_split_df.shape, \"units:\", len(train_units))\n",
    "print(\"Val split shape:\", val_split_df.shape, \"units:\", len(val_units))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9d43b3",
   "metadata": {},
   "source": [
    "EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2164aa64",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_rate = train_df.isna().mean().sort_values(ascending=False)\n",
    "print(\"Top missing rates:\")\n",
    "print(missing_rate.head(10))\n",
    "\n",
    "dup_count = train_df.duplicated().sum()\n",
    "print(\"Duplicate rows:\", dup_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57776ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_units = []\n",
    "for unit_id, g in train_df.groupby(\"unit_nr\", sort=False):\n",
    "    cycles = g[\"time_cycles\"].to_numpy()\n",
    "    cycles_sorted = np.sort(cycles)\n",
    "\n",
    "    diffs = cycles_sorted[1:] - cycles_sorted[:-1]\n",
    "    ok_start = cycles_sorted[0] == 1\n",
    "    ok_steps = (diffs.min() == 1) and (diffs.max() == 1)\n",
    "\n",
    "    if not (ok_start and ok_steps):\n",
    "        bad_units.append(unit_id)\n",
    "\n",
    "print(\"Units with non-continuous cycles:\", len(bad_units))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa72b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "lifetimes = train_df.groupby(\"unit_nr\")[\"time_cycles\"].max()\n",
    "print(lifetimes.describe())\n",
    "\n",
    "print(train_df[\"RUL\"].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b79c63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(lifetimes, bins=30)\n",
    "plt.title(\"Training set: lifetime distribution (cycles until failure)\")\n",
    "plt.xlabel(\"Lifetime (max cycles per engine)\")\n",
    "plt.ylabel(\"Number of engines\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ea458a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(train_df[\"RUL\"], bins=50)\n",
    "plt.title(\"Training set: RUL distribution (all cycles)\")\n",
    "plt.xlabel(\"RUL\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642a8ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fd_tag, g in train_df.groupby(\"fd\"):\n",
    "    lifetimes_fd = g.groupby(\"unit_nr\")[\"time_cycles\"].max()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.hist(lifetimes_fd, bins=30)\n",
    "    plt.title(f\"{fd_tag}: lifetime distribution\")\n",
    "    plt.xlabel(\"Lifetime (cycles)\")\n",
    "    plt.ylabel(\"Number of engines\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4aa6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = setting_names + sensor_names\n",
    "\n",
    "desc = train_split_df[feature_cols].describe().T\n",
    "desc[\"range\"] = desc[\"max\"] - desc[\"min\"]\n",
    "desc[\"std\"] = train_split_df[feature_cols].std(numeric_only=True)\n",
    "\n",
    "desc_sorted = desc.sort_values(\"range\")\n",
    "print(desc_sorted[[\"min\", \"max\", \"range\", \"std\"]].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acf1850",
   "metadata": {},
   "outputs": [],
   "source": [
    "range_thresh = 1e-6\n",
    "std_thresh = 1e-6\n",
    "\n",
    "low_variance = desc[(desc[\"range\"] <= range_thresh) | (desc[\"std\"] <= std_thresh)].index.tolist()\n",
    "print(\"Low-variance features:\", low_variance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20963d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs = {}\n",
    "sample_df = train_split_df.sample(n=min(len(train_split_df), 30000), random_state=SEED)\n",
    "\n",
    "for c in feature_cols:\n",
    "    corrs[c] = sample_df[c].corr(sample_df[\"RUL\"], method=\"spearman\")\n",
    "\n",
    "corr_s = pd.Series(corrs).sort_values(key=lambda x: x.abs(), ascending=False)\n",
    "print(\"Top correlations (abs Spearman):\")\n",
    "print(corr_s.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865d6c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "units_to_plot = np.random.default_rng(SEED).choice(train_df[\"unit_nr\"].unique(), size=3, replace=False).tolist()\n",
    "sensors_to_plot = [\"s_2\", \"s_3\", \"s_4\"]\n",
    "\n",
    "for unit_id in units_to_plot:\n",
    "    g = train_df[train_df[\"unit_nr\"] == unit_id].sort_values(\"time_cycles\")\n",
    "\n",
    "    plt.figure()\n",
    "    for s in sensors_to_plot:\n",
    "        plt.plot(g[\"time_cycles\"], g[s], label=s)\n",
    "\n",
    "    plt.title(f\"Unit {unit_id}: sensor trajectories\")\n",
    "    plt.xlabel(\"Cycle\")\n",
    "    plt.ylabel(\"Sensor value\")\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5e976e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "def collate_pad(batch: List[Tuple[torch.Tensor, float, int]]):\n",
    "    seqs, targets, lengths = zip(*batch)\n",
    "\n",
    "    lengths_t = torch.tensor(lengths, dtype=torch.long)\n",
    "    padded = pad_sequence(seqs, batch_first=True)\n",
    "    targets_t = torch.tensor(targets, dtype=torch.float32)\n",
    "\n",
    "    return padded.float(), lengths_t, targets_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b160e8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from dataset import CmapssRandomCropDataset\n",
    "\n",
    "def make_loader(sequences_by_unit, rul_by_unit, samples_per_epoch, batch_size, l_min, l_max, num_workers=0):\n",
    "    ds = CmapssRandomCropDataset(\n",
    "        sequences_by_unit=sequences_by_unit,\n",
    "        rul_by_unit=rul_by_unit,\n",
    "        samples_per_epoch=samples_per_epoch,\n",
    "        l_min=l_min,\n",
    "        l_max=l_max,\n",
    "    )\n",
    "\n",
    "    loader = DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        collate_fn=collate_pad,\n",
    "        drop_last=True,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    return loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6075c2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, device):\n",
    "    model.train()\n",
    "    loss_fn = nn.SmoothL1Loss()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    n = 0\n",
    "\n",
    "    for padded, lengths, targets in loader:\n",
    "        padded = padded.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        preds = model(padded, lengths)\n",
    "        loss = loss_fn(preds, targets)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        bs = targets.shape[0]\n",
    "        total_loss += loss.item() * bs\n",
    "        n += bs\n",
    "\n",
    "    return total_loss / max(n, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416b7661",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# The dataset has 26 columns\n",
    "# 1. Unit Number (Which engine is it?)\n",
    "# 2. Time Cycles (How long has it been running?)\n",
    "# 3-5. Operational Settings (Altitude, Speed, etc.)\n",
    "# 6-26. Sensor Readings (s1 to s21)\n",
    "\n",
    "index_names = ['unit_nr', 'time_cycles']\n",
    "setting_names = ['setting_1', 'setting_2', 'setting_3']\n",
    "sensor_names = ['s_{}'.format(i) for i in range(1, 22)] \n",
    "col_names = index_names + setting_names + sensor_names\n",
    "\n",
    "print(col_names)\n",
    "\n",
    "def load_fd(fd_tag):\n",
    "    train_path = f\"data/train_{fd_tag}.txt\"\n",
    "    test_path  = f\"data/test_{fd_tag}.txt\"\n",
    "    rul_path   = f\"data/RUL_{fd_tag}.txt\"\n",
    "\n",
    "    raw_train_df = pd.read_csv(train_path, sep=r'\\s+', header=None, names=col_names)\n",
    "    raw_test_df  = pd.read_csv(test_path,  sep=r'\\s+', header=None, names=col_names)\n",
    "    raw_rul_labels_df = pd.read_csv(rul_path, header=None, names=['RUL_truth'])\n",
    "\n",
    "    # train labels: compute RUL from run-to-failure\n",
    "    max_cycle = raw_train_df.groupby('unit_nr')['time_cycles'].max().rename('max_cycle')\n",
    "    raw_train_df = raw_train_df.merge(max_cycle, left_on='unit_nr', right_index=True)\n",
    "    raw_train_df['RUL'] = raw_train_df['max_cycle'] - raw_train_df['time_cycles']\n",
    "\n",
    "    # test labels: provided separately\n",
    "    return raw_train_df, raw_test_df, raw_rul_labels_df\n",
    "\n",
    "next_unit = 1\n",
    "train_dfs = []\n",
    "test_dfs = []\n",
    "test_rul_labels = []\n",
    "\n",
    "data_tags = [\"FD001\",\"FD002\",\"FD003\",\"FD004\"]\n",
    "data_tags = [\"FD001\", \"FD003\"]\n",
    "#data_tags = [\"FD001\"]\n",
    "\n",
    "for fd_tag in data_tags:\n",
    "    train_df_chunk, test_df_chunk, rul_labels_chunk = load_fd(fd_tag)\n",
    "    train_df_chunk['fd'] = fd_tag\n",
    "    test_df_chunk['fd'] = fd_tag\n",
    "    test_df_chunk = test_df_chunk.assign(\n",
    "        unit_nr_orig=test_df_chunk['unit_nr'],\n",
    "        unit_nr=test_df_chunk['unit_nr'] + next_unit - 1\n",
    "    )\n",
    "    test_dfs.append(test_df_chunk)\n",
    "    test_rul_labels.append(rul_labels_chunk)\n",
    "\n",
    "    # make a mapping for this FD's units\n",
    "    uniq_units = sorted(train_df_chunk['unit_nr'].unique())\n",
    "    mapping = {u: next_unit + i for i, u in enumerate(uniq_units)}\n",
    "    next_unit += len(uniq_units)\n",
    "\n",
    "    train_df_chunk = train_df_chunk.assign(\n",
    "        unit_nr_orig=train_df_chunk['unit_nr'],\n",
    "        unit_nr=train_df_chunk['unit_nr'].map(mapping),\n",
    "        fd=fd_tag\n",
    "    )\n",
    "    train_dfs.append(train_df_chunk)\n",
    "    \n",
    "    \n",
    "train_df = pd.concat(train_dfs, ignore_index=True)\n",
    "test_df = pd.concat(test_dfs, ignore_index=True)\n",
    "rul_labels_df = pd.concat(test_rul_labels, ignore_index=True)\n",
    "\n",
    "print(f\"Train data shape: {train_df.shape}\")\n",
    "print(f\"Test data shape: {test_df.shape}\")\n",
    "print(f\"RUL labels shape: {rul_labels_df.shape}\")\n",
    "\n",
    "engine_ids = train_df['unit_nr'].unique()\n",
    "\n",
    "train_ids = engine_ids[:150]\n",
    "val_ids = engine_ids[150:]\n",
    "print(\"len train ids:\", len(train_ids))\n",
    "print(\"len val ids:\", len(val_ids))\n",
    "print(\"len engine ids:\", len(engine_ids))\n",
    "\n",
    "train_split_df = train_df[train_df['unit_nr'].isin(train_ids)]\n",
    "val_split_df = train_df[train_df['unit_nr'].isin(val_ids)]\n",
    "print(\"Train df shape:\", train_split_df.shape)\n",
    "print(\"Val df shape:\", val_split_df.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99eaa923",
   "metadata": {},
   "source": [
    "EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab87e86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values per column\n",
    "missing = train_df.isna().mean().sort_values(ascending=False)\n",
    "print(missing.head(10))\n",
    "\n",
    "# Duplicate rows check\n",
    "dup_count = train_df.duplicated().sum()\n",
    "print(\"Duplicate rows:\", dup_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b10159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check cycle continuity per unit\n",
    "gaps = []\n",
    "for unit_id, g in train_df.groupby(\"unit_nr\"):\n",
    "    cycles = g[\"time_cycles\"].sort_values().to_numpy()\n",
    "    if not (cycles[0] == 1 and (cycles[1:] - cycles[:-1]).min() == 1):\n",
    "        gaps.append(unit_id)\n",
    "\n",
    "print(\"Units with non-continuous cycles:\", len(gaps))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4a842c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-unit lifetime distribution\n",
    "lifetimes = train_df.groupby(\"unit_nr\")[\"time_cycles\"].max()\n",
    "print(lifetimes.describe())\n",
    "\n",
    "# RUL distribution\n",
    "print(train_df[\"RUL\"].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5141c29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(lifetimes, bins=30)\n",
    "plt.title(\"Lifetime distribution (cycles until failure) - training set\")\n",
    "plt.xlabel(\"Lifetime (max cycles per engine)\")\n",
    "plt.ylabel(\"Number of engines\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d924dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "\n",
    "columns_to_scale = setting_names + sensor_names\n",
    "\n",
    "ranges = (train_split_df[columns_to_scale].max() - train_split_df[columns_to_scale].min()).sort_values(ascending=False)\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(np.arange(len(ranges)), ranges.to_numpy())\n",
    "plt.title(\"Per-feature range before scaling (train split)\")\n",
    "plt.xlabel(\"feature rank\")\n",
    "plt.ylabel(\"range\")\n",
    "plt.show()\n",
    "\n",
    "scaled_ranges = (scaled_train_split_df[columns_to_scale].max() - scaled_train_split_df[columns_to_scale].min()).sort_values(ascending=False)\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(np.arange(len(scaled_ranges)), scaled_ranges.to_numpy())\n",
    "plt.title(\"Per-feature range after MinMax scaling (train split)\")\n",
    "plt.xlabel(\"feature rank\")\n",
    "plt.ylabel(\"range\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1772a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eaa0d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rul_labels_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2ed57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "not_scaled_cols = ['unit_nr', 'RUL', 'max_cycle', 'time_cycles']\n",
    "\n",
    "col_set = set(col_names)\n",
    "columns_to_scale = [col for col in col_names if col not in not_scaled_cols]\n",
    "\n",
    "print(\"Columns to scale:\", columns_to_scale)\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_train_split = scaler.fit_transform(train_split_df[columns_to_scale])\n",
    "scaled_train_split_df = pd.DataFrame(scaled_train_split, columns=columns_to_scale, index=train_split_df.index)\n",
    "scaled_val_split = scaler.transform(val_split_df[columns_to_scale])\n",
    "scaled_val_split_df = pd.DataFrame(scaled_val_split, columns=columns_to_scale, index=val_split_df.index)\n",
    "scaled_test = scaler.transform(test_df[columns_to_scale])\n",
    "scaled_test_df = pd.DataFrame(scaled_test, columns=columns_to_scale, index=test_df.index)\n",
    "\n",
    "\n",
    "scaled_train_split_df.insert(0, 'unit_nr', train_split_df['unit_nr'])\n",
    "scaled_train_split_df.insert(1, 'time_cycles', train_split_df['time_cycles'])\n",
    "scaled_train_split_df.insert(len(scaled_train_split_df.columns), 'RUL', train_split_df['RUL'])\n",
    "scaled_train_split_df.insert(len(scaled_train_split_df.columns), 'max_cycle', train_split_df['max_cycle'])\n",
    "\n",
    "scaled_val_split_df.insert(0, 'unit_nr', val_split_df['unit_nr'])\n",
    "scaled_val_split_df.insert(1, 'time_cycles', val_split_df['time_cycles'])\n",
    "scaled_val_split_df.insert(len(scaled_val_split_df.columns), 'RUL', val_split_df['RUL'])\n",
    "scaled_val_split_df.insert(len(scaled_val_split_df.columns), 'max_cycle', val_split_df['max_cycle'])\n",
    "\n",
    "scaled_test_df.insert(0, 'unit_nr', test_df['unit_nr'])\n",
    "scaled_test_df.insert(1, 'time_cycles', test_df['time_cycles'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439c198c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b35874",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_train_split_df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab20815",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_val_split_df.info()\n",
    "scaled_val_split_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231e7ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_by_unit = {}\n",
    "rul_by_unit = {}\n",
    "\n",
    "#feature_cols = col_names[2:]  # Exclude 'unit_nr' and 'time_cycles'\n",
    "features_to_drop = ['unit_nr', 'time_cycles', 'RUL', 'max_cycle', \"s_1\", \"s_5\", \"s_10\", \"s_16\", \"s_18\", \"s_19\"]\n",
    "feature_cols = [c for c in col_names if c not in features_to_drop]\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "def build_unit_dicts(df, feature_cols):\n",
    "    sequences_by_unit = {}\n",
    "    rul_by_unit = {}\n",
    "\n",
    "    df = df.sort_values([\"unit_nr\", \"time_cycles\"])\n",
    "\n",
    "    for unit_id, g in df.groupby(\"unit_nr\", sort=False):\n",
    "        x = torch.tensor(g[feature_cols].to_numpy(), dtype=torch.float32)\n",
    "        y = torch.tensor(g[\"RUL\"].to_numpy(), dtype=torch.float32)\n",
    "\n",
    "        sequences_by_unit[int(unit_id)] = x\n",
    "        rul_by_unit[int(unit_id)] = y\n",
    "\n",
    "    return sequences_by_unit, rul_by_unit\n",
    "\n",
    "train_sequences_by_unit, train_rul_by_unit = build_unit_dicts(scaled_train_split_df, feature_cols)\n",
    "val_sequences_by_unit, val_rul_by_unit = build_unit_dicts(scaled_val_split_df, feature_cols)\n",
    "\n",
    "print(\"train_sequences_by_unit\", len(train_sequences_by_unit))\n",
    "\n",
    "from dataset import CmapssRandomCropDataset\n",
    "\n",
    "dataset = CmapssRandomCropDataset(\n",
    "    sequences_by_unit=train_sequences_by_unit,\n",
    "    rul_by_unit=train_rul_by_unit,\n",
    "    samples_per_epoch=1000,\n",
    "    l_min=30,\n",
    "    l_max=100,\n",
    ")\n",
    "\n",
    "seq, target, length = dataset[0]  # or dataset[42], same behavior\n",
    "print(seq.shape, target, length)\n",
    "\n",
    "train_loader = make_loader(train_sequences_by_unit, train_rul_by_unit, samples_per_epoch=60000, batch_size=32, l_min=30, l_max=100)\n",
    "padded, lengths, targets = next(iter(train_loader))\n",
    "print(padded.shape, lengths.shape, targets.shape)\n",
    "\n",
    "val_loader = make_loader(val_sequences_by_unit, val_rul_by_unit, samples_per_epoch=14000, batch_size=32, l_min=30, l_max=200)\n",
    "padded, lengths, targets = next(iter(val_loader))\n",
    "print(padded.shape, lengths.shape, targets.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc7caff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def build_test_sequences(df, feature_cols):\n",
    "    sequences = {}\n",
    "    df = df.sort_values([\"unit_nr\", \"time_cycles\"])\n",
    "\n",
    "    for unit_id, g in df.groupby(\"unit_nr\", sort=False):\n",
    "        x = torch.tensor(g[feature_cols].to_numpy(), dtype=torch.float32)\n",
    "        sequences[int(unit_id)] = x\n",
    "\n",
    "    return sequences\n",
    "\n",
    "test_sequences_by_unit = build_test_sequences(scaled_test_df, feature_cols)\n",
    "\n",
    "rul_truth = rul_labels_df[\"RUL_truth\"].to_numpy().astype(float)\n",
    "test_unit_ids = sorted(test_sequences_by_unit.keys())\n",
    "\n",
    "test_targets_by_unit = {unit_id: float(rul_truth[i]) for i, unit_id in enumerate(test_unit_ids)}\n",
    "print(\"test_sequences_by_unit\", len(test_sequences_by_unit))\n",
    "print(\"test_sequences_by_unit[0]\", test_sequences_by_unit[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb6bf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CmapssLastWindowDataset(Dataset):\n",
    "    def __init__(self, sequences_by_unit, targets_by_unit, window_len: int):\n",
    "        self.sequences_by_unit = sequences_by_unit\n",
    "        self.targets_by_unit = targets_by_unit\n",
    "        self.window_len = window_len\n",
    "        self.unit_ids = sorted(sequences_by_unit.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.unit_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        unit_id = self.unit_ids[idx]\n",
    "        seq = self.sequences_by_unit[unit_id]\n",
    "        target = self.targets_by_unit[unit_id]\n",
    "\n",
    "        if self.window_len is not None and seq.shape[0] > self.window_len:\n",
    "            seq = seq[-self.window_len:]\n",
    "\n",
    "        length = int(seq.shape[0])\n",
    "        return seq, float(target), length\n",
    "    \n",
    "class CmapssFullDataset(Dataset):\n",
    "    def __init__(self, sequences_by_unit, targets_by_unit):\n",
    "        self.sequences_by_unit = sequences_by_unit\n",
    "        self.targets_by_unit = targets_by_unit\n",
    "        self.unit_ids = sorted(sequences_by_unit.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.unit_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        unit_id = self.unit_ids[idx]\n",
    "        seq = self.sequences_by_unit[unit_id]\n",
    "        target = self.targets_by_unit[unit_id]\n",
    "\n",
    "        length = int(seq.shape[0])\n",
    "        return seq, float(target), length\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5861de0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "WINDOW_LEN = 100\n",
    "\n",
    "test_ds = CmapssLastWindowDataset(\n",
    "    sequences_by_unit=test_sequences_by_unit,\n",
    "    targets_by_unit=test_targets_by_unit,\n",
    "    window_len=WINDOW_LEN,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_pad,\n",
    "    drop_last=False,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "test_ds = CmapssFullDataset(\n",
    "    sequences_by_unit=test_sequences_by_unit,\n",
    "    targets_by_unit=test_targets_by_unit,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_pad,\n",
    "    drop_last=False,\n",
    "    pin_memory=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1078abee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def run_epoch(model, loader, loss_fn, device, train: bool, optimizer=None):\n",
    "    if train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    if train:\n",
    "        context = torch.enable_grad()\n",
    "    else:\n",
    "        context = torch.no_grad()\n",
    "\n",
    "    with context:\n",
    "        for padded, lengths, targets in loader:\n",
    "            padded = padded.to(device)\n",
    "            lengths = lengths.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            preds = model(padded, lengths)\n",
    "            loss = loss_fn(preds, targets)\n",
    "\n",
    "            bs = targets.size(0)\n",
    "            total_loss += loss.item() * bs\n",
    "            total_samples += bs\n",
    "\n",
    "            if train:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "\n",
    "    return total_loss / max(total_samples, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cd92c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "EPOCHS = 20\n",
    "\n",
    "if not os.path.exists('models'):\n",
    "    os.makedirs('models')\n",
    "\n",
    "def train_full(model, train_loader, val_loader, optimizer, device):\n",
    "    loss_fn = nn.SmoothL1Loss(reduction=\"mean\")\n",
    "\n",
    "    best_val = float(\"inf\")\n",
    "    best_state = None\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss = run_epoch(\n",
    "            model=model,\n",
    "            loader=train_loader,\n",
    "            loss_fn=loss_fn,\n",
    "            device=device,\n",
    "            train=True,\n",
    "            optimizer=optimizer,\n",
    "        )\n",
    "\n",
    "        val_loss = run_epoch(\n",
    "            model=model,\n",
    "            loader=val_loader,\n",
    "            loss_fn=loss_fn,\n",
    "            device=device,\n",
    "            train=False,\n",
    "            optimizer=None,\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch+1}/{EPOCHS} \"\n",
    "            f\"train_loss: {train_loss:.4f} | val_loss: {val_loss:.4f}\"\n",
    "        )\n",
    "\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            best_state = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "        torch.save(best_state, 'models/lstm_model.pth')\n",
    "        print(\"Model weights saved to models/lstm_model.pth\")\n",
    "\n",
    "    return best_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dd232a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import RulLstm\n",
    "\n",
    "model = RulLstm(\n",
    "    n_features=len(feature_cols),\n",
    "    hidden_size=128,\n",
    "    num_layers=2,\n",
    "    dropout=0.4,\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-3, weight_decay=1e-4)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "model.to(device)\n",
    "\n",
    "#loss = train_one_epoch(model, dataset_loader, optimizer, device)\n",
    "\n",
    "loss = train_full(model, train_loader, val_loader, optimizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c044a9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(\"cpu\")\n",
    "model.eval()\n",
    "\n",
    "x = torch.randn(32, 217, 18)\n",
    "lengths = torch.full((32,), 217, dtype=torch.long)  # or whatever your lengths are\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    (x, lengths),\n",
    "    \"models/lstm_model.onnx\",\n",
    "    input_names=[\"input\", \"lengths\"],\n",
    "    output_names=[\"output\"],\n",
    "    opset_version=18,\n",
    "    dynamic_axes={\n",
    "        \"input\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "        \"lengths\": {0: \"batch_size\"},\n",
    "    },\n",
    "    dynamo=False,  # optional but avoids dynamo issues\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bae36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "\n",
    "sess = ort.InferenceSession(\"models/lstm_model.onnx\", providers=[\"CPUExecutionProvider\"])\n",
    "\n",
    "x = np.random.randn(1, 30, 18).astype(np.float32)\n",
    "lengths = np.array([30], dtype=np.int64)\n",
    "y = sess.run(None, {\"input\": x, \"lengths\": lengths})\n",
    "print(\"Inference successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac43b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "\n",
    "sess = ort.InferenceSession(\"models/lstm_model.onnx\", providers=[\"CPUExecutionProvider\"])\n",
    "\n",
    "#x = np.random.randn(1, 30, 18).astype(np.float32)\n",
    "#lengths = np.array([30], dtype=np.int64)\n",
    "\n",
    "seq, target, length = test_ds[20]\n",
    "\n",
    "print(\"Evaluating test sequence with shape:\", seq.shape)\n",
    "print(\"Length:\", length)\n",
    "print(\"Target RUL:\", target)\n",
    "\n",
    "seq = seq.numpy().reshape(1, -1, seq.shape[1])\n",
    "\n",
    "lengths = np.array([length], dtype=np.int64)\n",
    "outputs = sess.run(None, {\"input\": seq, \"lengths\": lengths})\n",
    "\n",
    "print(\"Number of outputs:\", len(outputs))\n",
    "for i, out in enumerate(outputs):\n",
    "    print(f\"Output[{i}] shape:\", out.shape, \"dtype:\", out.dtype)\n",
    "    print(f\"Output[{i}] data:\", out.item())  # Print the actual output data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f07dd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "\n",
    "    smoothl1 = nn.SmoothL1Loss(reduction=\"mean\")\n",
    "    l1 = nn.L1Loss(reduction=\"mean\")\n",
    "    mse = nn.MSELoss(reduction=\"mean\")\n",
    "\n",
    "    total_smoothl1 = 0.0\n",
    "    total_l1 = 0.0\n",
    "    total_mse = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    for padded, lengths, targets in loader:\n",
    "        padded = padded.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        print(\"Evaluating batch with padded shape:\", padded.shape)\n",
    "        print(\"Lengths:\", lengths)\n",
    "        preds = model(padded, lengths)\n",
    "\n",
    "        bs = targets.size(0)\n",
    "        total_smoothl1 += smoothl1(preds, targets).item() * bs\n",
    "        total_l1 += l1(preds, targets).item() * bs\n",
    "        total_mse += mse(preds, targets).item() * bs\n",
    "        total_samples += bs\n",
    "\n",
    "        all_preds.append(preds.detach().cpu())\n",
    "        all_targets.append(targets.detach().cpu())\n",
    "\n",
    "    mean_smoothl1 = total_smoothl1 / max(total_samples, 1)\n",
    "    mean_mae = total_l1 / max(total_samples, 1)\n",
    "    mean_rmse = float(np.sqrt(total_mse / max(total_samples, 1)))\n",
    "\n",
    "    preds_cat = torch.cat(all_preds).numpy()\n",
    "    targets_cat = torch.cat(all_targets).numpy()\n",
    "\n",
    "    return mean_smoothl1, mean_mae, mean_rmse, preds_cat, targets_cat\n",
    "\n",
    "smoothl1_loss, mae, rmse, preds, targets = evaluate(model, test_loader, device)\n",
    "\n",
    "print(f\"Test SmoothL1: {smoothl1_loss:.4f}\")\n",
    "print(f\"Test MAE:      {mae:.4f}\")\n",
    "print(f\"Test RMSE:     {rmse:.4f}\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(targets, label=\"True RUL\")\n",
    "plt.plot(preds, label=\"Pred RUL\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b7fb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "@torch.no_grad()\n",
    "def plot_single_test_unit_last_prediction(\n",
    "    model,\n",
    "    scaled_test_df,\n",
    "    rul_labels_df,\n",
    "    feature_cols,\n",
    "    unit_id: int,\n",
    "    device: torch.device,\n",
    "):\n",
    "    g = scaled_test_df[scaled_test_df[\"unit_nr\"] == unit_id].sort_values(\"time_cycles\")\n",
    "    if len(g) == 0:\n",
    "        raise ValueError(f\"unit_id {unit_id} not found in scaled_test_df\")\n",
    "\n",
    "    full_seq = torch.tensor(g[feature_cols].to_numpy(), dtype=torch.float32)\n",
    "    cycles = g[\"time_cycles\"].to_numpy().astype(int)\n",
    "\n",
    "    last_cycle = int(cycles[-1])\n",
    "\n",
    "    test_unit_ids = sorted(scaled_test_df[\"unit_nr\"].unique().tolist())\n",
    "    unit_idx = test_unit_ids.index(unit_id)\n",
    "\n",
    "    true_rul_last = float(rul_labels_df[\"RUL_truth\"].iloc[unit_idx])\n",
    "\n",
    "    failure_cycle = last_cycle + true_rul_last\n",
    "    true_rul_curve = (failure_cycle - cycles).astype(float)\n",
    "\n",
    "    padded = full_seq.unsqueeze(0).to(device)\n",
    "    lengths = torch.tensor([full_seq.shape[0]], dtype=torch.long, device=device)\n",
    "\n",
    "    model.eval()\n",
    "    pred_rul_last = float(model(padded, lengths)[0].item())\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(cycles, true_rul_curve, label=\"True RUL (derived)\")\n",
    "    plt.scatter([last_cycle], [true_rul_last], label=\"True RUL at last cycle\")\n",
    "    plt.scatter([last_cycle], [pred_rul_last], label=\"Predicted RUL at last cycle\")\n",
    "    plt.xlabel(\"Cycle\")\n",
    "    plt.ylabel(\"RUL\")\n",
    "    plt.title(f\"Test unit {unit_id} | last_cycle={last_cycle} | true_last={true_rul_last:.2f} | pred_last={pred_rul_last:.2f}\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return pred_rul_last, true_rul_last\n",
    "\n",
    "pred_last, true_last = plot_single_test_unit_last_prediction(\n",
    "    model=model,\n",
    "    scaled_test_df=scaled_test_df,\n",
    "    rul_labels_df=rul_labels_df,\n",
    "    feature_cols=feature_cols,\n",
    "    unit_id=3,\n",
    "    device=device,\n",
    ")\n",
    "print(\"pred_last:\", pred_last, \"true_last:\", true_last)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cda84c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_test_unit_ids(scaled_test_df):\n",
    "    return sorted(scaled_test_df[\"unit_nr\"].unique().tolist())\n",
    "\n",
    "def show_unit(model, scaled_test_df, rul_labels_df, feature_cols, device, unit_id):\n",
    "    pred_last, true_last = plot_single_test_unit_last_prediction(\n",
    "        model=model,\n",
    "        scaled_test_df=scaled_test_df,\n",
    "        rul_labels_df=rul_labels_df,\n",
    "        feature_cols=feature_cols,\n",
    "        unit_id=unit_id,\n",
    "        device=device,\n",
    "    )\n",
    "    print(f\"unit_id={unit_id} pred_last={pred_last:.2f} true_last={true_last:.2f}\")\n",
    "\n",
    "unit_ids = list_test_unit_ids(scaled_test_df)\n",
    "show_unit(model, scaled_test_df, rul_labels_df, feature_cols, device, unit_ids[0])\n",
    "show_unit(model, scaled_test_df, rul_labels_df, feature_cols, device, unit_ids[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4137e63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "@torch.no_grad()\n",
    "def plot_many_units_last_prediction(\n",
    "    model,\n",
    "    scaled_test_df,\n",
    "    rul_labels_df,\n",
    "    feature_cols,\n",
    "    device,\n",
    "    unit_ids,\n",
    "    cols=3,\n",
    "):\n",
    "    model.eval()\n",
    "\n",
    "    test_unit_ids_all = sorted(scaled_test_df[\"unit_nr\"].unique().tolist())\n",
    "\n",
    "    n = len(unit_ids)\n",
    "    rows = int(math.ceil(n / cols))\n",
    "    plt.figure(figsize=(5 * cols, 4 * rows))\n",
    "\n",
    "    for i, unit_id in enumerate(unit_ids, start=1):\n",
    "        g = scaled_test_df[scaled_test_df[\"unit_nr\"] == unit_id].sort_values(\"time_cycles\")\n",
    "        if len(g) == 0:\n",
    "            continue\n",
    "\n",
    "        full_seq = torch.tensor(g[feature_cols].to_numpy(), dtype=torch.float32)\n",
    "        cycles = g[\"time_cycles\"].to_numpy().astype(int)\n",
    "        last_cycle = int(cycles[-1])\n",
    "\n",
    "        unit_idx = test_unit_ids_all.index(unit_id)\n",
    "        true_rul_last = float(rul_labels_df[\"RUL_truth\"].iloc[unit_idx])\n",
    "\n",
    "        failure_cycle = last_cycle + true_rul_last\n",
    "        true_rul_curve = (failure_cycle - cycles).astype(float)\n",
    "\n",
    "        padded = full_seq.unsqueeze(0).to(device)\n",
    "        lengths = torch.tensor([full_seq.shape[0]], dtype=torch.long, device=device)\n",
    "        pred_rul_last = float(model(padded, lengths)[0].item())\n",
    "\n",
    "        ax = plt.subplot(rows, cols, i)\n",
    "        ax.plot(cycles, true_rul_curve, label=\"True\")\n",
    "        ax.scatter([last_cycle], [true_rul_last], label=\"True last\")\n",
    "        ax.scatter([last_cycle], [pred_rul_last], label=\"Pred last\")\n",
    "        ax.set_title(f\"Unit {unit_id} | true={true_rul_last:.1f} pred={pred_rul_last:.1f}\")\n",
    "        ax.set_xlabel(\"Cycle\")\n",
    "        ax.set_ylabel(\"RUL\")\n",
    "\n",
    "        if i == 1:\n",
    "            ax.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "unit_ids = sorted(scaled_test_df[\"unit_nr\"].unique())\n",
    "plot_many_units_last_prediction(\n",
    "    model=model,\n",
    "    scaled_test_df=scaled_test_df,\n",
    "    rul_labels_df=rul_labels_df,\n",
    "    feature_cols=feature_cols,\n",
    "    device=device,\n",
    "    unit_ids=unit_ids[:9],\n",
    "    cols=3,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20f3d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from model import RulLstm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def run_trial(params, seed=0):\n",
    "    set_seed(seed)\n",
    "\n",
    "    train_loader = make_loader(\n",
    "        train_sequences_by_unit,\n",
    "        train_rul_by_unit,\n",
    "        samples_per_epoch=params[\"samples_per_epoch\"],\n",
    "        batch_size=params[\"batch_size\"],\n",
    "        l_min=params[\"l_min\"],\n",
    "        l_max=params[\"l_max\"],\n",
    "    )\n",
    "    val_loader = make_loader(\n",
    "        val_sequences_by_unit,\n",
    "        val_rul_by_unit,\n",
    "        samples_per_epoch=params.get(\"val_samples_per_epoch\", 14000),\n",
    "        batch_size=params[\"batch_size\"],\n",
    "        l_min=params[\"l_min\"],\n",
    "        l_max=params.get(\"val_l_max\", params[\"l_max\"]),\n",
    "    )\n",
    "\n",
    "    model = RulLstm(\n",
    "        n_features=len(feature_cols),\n",
    "        hidden_size=params[\"hidden_size\"],\n",
    "        num_layers=params[\"num_layers\"],\n",
    "        dropout=params[\"dropout\"],\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=params[\"lr\"],\n",
    "        weight_decay=params[\"weight_decay\"],\n",
    "    )\n",
    "\n",
    "    return train_full(model, train_loader, val_loader, optimizer, device)\n",
    "\n",
    "search_space = {\n",
    "    \"hidden_size\": [64, 128, 256],\n",
    "    \"num_layers\": [1, 2, 3],\n",
    "    \"dropout\": [0.0,0.1, 0.3, 0.5],\n",
    "    \"lr\": [5e-4, 1e-3, 2e-3],\n",
    "    \"weight_decay\": [0.0, 1e-4],\n",
    "    \"batch_size\": [16, 32, 64],\n",
    "    \"samples_per_epoch\": [20000, 60000],\n",
    "    \"l_min\": [20, 30],\n",
    "    \"l_max\": [100, 150, 200],\n",
    "}\n",
    "\n",
    "\n",
    "#best: (10.090411104202271, {'hidden_size': 128, 'num_layers': 1, 'dropout': 0.3, 'lr': 0.001, 'weight_decay': 0.0, 'batch_size': 16, 'samples_per_epoch': 60000, 'l_min': 30, 'l_max': 200})\n",
    "\n",
    "search_space = {\n",
    "    \"hidden_size\": [64],\n",
    "    \"num_layers\": [1, 2],\n",
    "    \"dropout\": [0.0,0.3],\n",
    "    \"lr\": [1e-3, 2e-3, 5e-4],\n",
    "    \"weight_decay\": [0.0, 1e-4],\n",
    "    \"batch_size\": [16, 32],\n",
    "    \"samples_per_epoch\": [60000],\n",
    "    \"l_min\": [30],\n",
    "    \"l_max\": [200],\n",
    "}\n",
    "\n",
    "keys = list(search_space.keys())\n",
    "candidates = [dict(zip(keys, vals)) for vals in itertools.product(*(search_space[k] for k in keys))]\n",
    "#random.shuffle(candidates)\n",
    "\n",
    "EPOCHS = 15  # fast sweep; raise later for final training\n",
    "best = (float(\"inf\"), None)\n",
    "results = []\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "results_path = Path(\"tuning_results.txt\")\n",
    "with results_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"trial\\tval_loss\\tparams\\n\")\n",
    "\n",
    "    for i, params in enumerate(candidates):\n",
    "        print(f\"Trial {i+1}/{len(candidates)} with params: {params}\")\n",
    "        val_loss = run_trial(params, seed=123 + i)\n",
    "        results.append({**params, \"val_loss\": val_loss})\n",
    "        if val_loss < best[0]:\n",
    "            best = (val_loss, params)\n",
    "\n",
    "        f.write(f\"{i+1}\\t{val_loss:.6f}\\t{params}\\n\")\n",
    "        f.flush()\n",
    "\n",
    "print(\"best:\", best)\n",
    "\n",
    "#best: (8.913974617958068, {'hidden_size': 64, 'num_layers': 1, 'dropout': 0.3, 'lr': 0.001, 'weight_decay': 0.0, 'batch_size': 16, 'samples_per_epoch': 60000, 'l_min': 30, 'l_max': 200})\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gemini-lstm-next-frame-prediction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
