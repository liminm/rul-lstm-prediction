{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3042b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 1. Create a directory for the data\n",
    "if not os.path.exists('data'):\n",
    "    os.makedirs('data')\n",
    "\n",
    "# 2. Download the zip file (using a reliable mirror for the NASA dataset)\n",
    "!#wget https://data.nasa.gov/docs/legacy/CMAPSSData.zip -O data/CMAPSSData.zip\n",
    "\n",
    "# 3. Unzip it\n",
    "#!unzip -o data/CMAPSSData.zip -d data/\n",
    "\n",
    "#print(\"Data downloaded and extracted!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f394930",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f630136c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset has 26 columns\n",
    "# 1. Unit Number (Which engine is it?)\n",
    "# 2. Time Cycles (How long has it been running?)\n",
    "# 3-5. Operational Settings (Altitude, Speed, etc.)\n",
    "# 6-26. Sensor Readings (s1 to s21)\n",
    "\n",
    "index_names = ['unit_nr', 'time_cycles']\n",
    "setting_names = ['setting_1', 'setting_2', 'setting_3']\n",
    "sensor_names = ['s_{}'.format(i) for i in range(1, 22)] \n",
    "col_names = index_names + setting_names + sensor_names\n",
    "\n",
    "print(col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b277cdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fd(fd_tag):\n",
    "    train_path = f\"data/train_{fd_tag}.txt\"\n",
    "    test_path  = f\"data/test_{fd_tag}.txt\"\n",
    "    rul_path   = f\"data/RUL_{fd_tag}.txt\"\n",
    "\n",
    "    raw_train_df = pd.read_csv(train_path, sep=r'\\s+', header=None, names=col_names)\n",
    "    raw_test_df  = pd.read_csv(test_path,  sep=r'\\s+', header=None, names=col_names)\n",
    "    raw_rul_labels_df = pd.read_csv(rul_path, header=None, names=['RUL_truth'])\n",
    "\n",
    "    # train labels: compute RUL from run-to-failure\n",
    "    max_cycle = raw_train_df.groupby('unit_nr')['time_cycles'].max().rename('max_cycle')\n",
    "    raw_train_df = raw_train_df.merge(max_cycle, left_on='unit_nr', right_index=True)\n",
    "    raw_train_df['RUL'] = raw_train_df['max_cycle'] - raw_train_df['time_cycles']\n",
    "\n",
    "    # test labels: provided separately\n",
    "    return raw_train_df, raw_test_df, raw_rul_labels_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383c68f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_unit = 1\n",
    "train_dfs = []\n",
    "test_dfs = []\n",
    "test_rul_labels = []\n",
    "\n",
    "data_tags = [\"FD001\",\"FD002\",\"FD003\",\"FD004\"]\n",
    "\n",
    "for fd_tag in data_tags:\n",
    "    train_df_chunk, test_df_chunk, rul_labels_chunk = load_fd(fd_tag)\n",
    "    train_df_chunk['fd'] = fd_tag\n",
    "    test_df_chunk['fd'] = fd_tag\n",
    "    test_df_chunk = test_df_chunk.assign(\n",
    "        unit_nr_orig=test_df_chunk['unit_nr'],\n",
    "        unit_nr=test_df_chunk['unit_nr'] + next_unit - 1\n",
    "    )\n",
    "    test_dfs.append(test_df_chunk)\n",
    "    test_rul_labels.append(rul_labels_chunk)\n",
    "\n",
    "    # make a mapping for this FD's units\n",
    "    uniq_units = sorted(train_df_chunk['unit_nr'].unique())\n",
    "    mapping = {u: next_unit + i for i, u in enumerate(uniq_units)}\n",
    "    next_unit += len(uniq_units)\n",
    "\n",
    "    train_df_chunk = train_df_chunk.assign(\n",
    "        unit_nr_orig=train_df_chunk['unit_nr'],\n",
    "        unit_nr=train_df_chunk['unit_nr'].map(mapping),\n",
    "        fd=fd_tag\n",
    "    )\n",
    "    train_dfs.append(train_df_chunk)\n",
    "    \n",
    "    \n",
    "data_df = pd.concat(train_dfs, ignore_index=True)\n",
    "train_df = pd.concat(train_dfs, ignore_index=True)\n",
    "test_df = pd.concat(test_dfs, ignore_index=True)\n",
    "rul_labels_df = pd.concat(test_rul_labels, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d882b36",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b337b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23dc1971",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc399df",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59062ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rul_labels_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76d4357",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "not_scaled_cols = ['unit_nr', 'RUL', 'max_cycle', 'time_cycles']\n",
    "\n",
    "col_set = set(col_names)\n",
    "columns_to_scale = [col for col in col_names if col not in not_scaled_cols]\n",
    "\n",
    "print(\"Columns to scale:\", columns_to_scale)\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled = scaler.fit_transform(train_df[columns_to_scale])\n",
    "scaled_data_df = pd.DataFrame(scaled, columns=columns_to_scale, index=train_df.index)\n",
    "\n",
    "scaled_data_df.insert(0, 'unit_nr', train_df['unit_nr'])\n",
    "scaled_data_df.insert(1, 'time_cycles', train_df['time_cycles'])\n",
    "scaled_data_df.insert(len(scaled_data_df.columns), 'RUL', train_df['RUL'])\n",
    "scaled_data_df.insert(len(scaled_data_df.columns), 'max_cycle', train_df['max_cycle'])\n",
    "scaled_data_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff68924c",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine_ids = scaled_data_df['unit_nr'].unique()\n",
    "engine_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd502a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = engine_ids[:650]\n",
    "test_ids = engine_ids[650:]\n",
    "print(\"len train ids:\", len(train_ids))\n",
    "print(\"len test ids:\", len(test_ids))\n",
    "print(\"len engine ids:\", len(engine_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed9cb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = scaled_data_df[scaled_data_df['unit_nr'].isin(train_ids)]\n",
    "test_df = scaled_data_df[scaled_data_df['unit_nr'].isin(test_ids)]\n",
    "print(\"Train df shape:\", train_df.shape)\n",
    "print(\"Test df shape:\", test_df.shape)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c990fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d4b657",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "\n",
    "def create_sequences_vectorized(X, y, unit_ids, seq_length=50):\n",
    "    # 1. Create windows (Batch, Seq, Features)\n",
    "    X_windows = sliding_window_view(X, window_shape=seq_length, axis=0)\n",
    "    X_windows = X_windows.transpose(0, 2, 1) # (Batch, Seq, Features)\n",
    "    \n",
    "    # 2. Align Targets (End of window)\n",
    "    y_aligned = y[seq_length-1:]\n",
    "    \n",
    "    # 3. Create Mask (Ensure window doesn't cross units)\n",
    "    unit_ids_start = unit_ids[:-seq_length+1]\n",
    "    unit_ids_end   = unit_ids[seq_length-1:]\n",
    "    valid_mask = (unit_ids_start == unit_ids_end)\n",
    "    \n",
    "    return X_windows[valid_mask], y_aligned[valid_mask]\n",
    "\n",
    "# --- Usage ---\n",
    "sensor_cols = [c for c in train_df.columns if c.startswith('s_')]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 1. Prepare arrays (Sorted)\n",
    "train_df = train_df.sort_values(['unit_nr', 'time_cycles'])\n",
    "test_df = test_df.sort_values(['unit_nr', 'time_cycles'])\n",
    "\n",
    "\n",
    "# Apply rolling mean with window 9 (common for FD001)\n",
    "#print(\"Smoothing sensor data...\")\n",
    "for col in sensor_cols:\n",
    "    train_df[col] = train_df.groupby('unit_nr')[col].transform(\n",
    "    lambda x: x.rolling(window=9, min_periods=1).mean())\n",
    "    test_df[col] = test_df.groupby('unit_nr')[col].transform(\n",
    "    lambda x: x.rolling(window=9, min_periods=1).mean())\n",
    "\n",
    "\n",
    "# 2. CRITICAL: Drop Target (RUL) and max_cycle from Inputs\n",
    "# We only want the 24 sensor/setting columns + time_cycle\n",
    "features_to_drop = ['unit_nr', 'time_cycles', 'RUL', 'max_cycle', \"s_1\", \"s_5\", \"s_10\", \"s_16\", \"s_18\", \"s_19\"]\n",
    "\n",
    "X_train_arr = train_df.drop(columns=features_to_drop).values\n",
    "y_train_arr = train_df['RUL'].values \n",
    "train_units = train_df['unit_nr'].values\n",
    "\n",
    "X_test_arr = test_df.drop(columns=features_to_drop).values\n",
    "y_test_arr = test_df['RUL'].values\n",
    "test_units = test_df['unit_nr'].values\n",
    "\n",
    "# 3. Create Sequences\n",
    "X_train_seq, y_train_seq = create_sequences_vectorized(X_train_arr, y_train_arr, train_units, 50)\n",
    "X_test_seq, y_test_seq = create_sequences_vectorized(X_test_arr, y_test_arr, test_units, 50)\n",
    "\n",
    "print(f\"Train Input Shape: {X_train_seq.shape}\") \n",
    "# Expected shape: (N, 50, 24) -> 24 features\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train_seq, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_seq, dtype=torch.float32)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test_seq, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test_seq, dtype=torch.float32)   \n",
    "\n",
    "print(f\"X_train_tensor.shape: {X_train_tensor.shape}\")\n",
    "print(f\"y_train_tensor.shape: {y_train_tensor.shape}\")\n",
    "\n",
    "print(f\"X_test_tensor.shape: {X_test_tensor.shape}\")\n",
    "print(f\"y_test_tensor.shape: {y_test_tensor.shape}\")\n",
    "#print(f\"X_test_tensor[0]: {X_test_tensor[0]}\")\n",
    "#print(f\"y_test_tensor[0]: {y_test_tensor[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9b0ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch import nn\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import copy\n",
    "from model import EngineRULPredictor\n",
    "\n",
    "# 2. Setup (Reduced hidden size slightly to 128 to prevent overfitting)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "\n",
    "model = EngineRULPredictor(input_size=X_train_tensor.shape[2], hidden_size=512, num_layers=2, dropout=0.2)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=5\n",
    ")\n",
    "\n",
    "# 3. Training with \"Save Best\" logic\n",
    "EPOCHS = 10  # Lower epochs, let early stopping do the work\n",
    "best_test_rmse = float('inf')\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "max_rul = train_df['RUL'].max()\n",
    "\n",
    "y_train_tensor = torch.tensor(y_train_seq / max_rul, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test_seq / max_rul, dtype=torch.float32)\n",
    "\n",
    "# Move validation data to GPU once\n",
    "X_test_gpu = X_test_tensor.to(device)\n",
    "y_test_real = y_test_seq # Keep real values for RMSE calculation\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    TensorDataset(X_train_tensor, y_train_tensor),\n",
    "    batch_size=256,\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Starting Training with Validation...\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train() # Set to training mode (enables Dropout)\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(X_batch)\n",
    "        loss = criterion(out.squeeze(), y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    # --- Validation Step ---\n",
    "    model.eval() # Set to eval mode (disables Dropout)\n",
    "    with torch.no_grad():\n",
    "        # Get predictions\n",
    "        preds_scaled = model(X_test_gpu).cpu().numpy().flatten()\n",
    "        # Unscale\n",
    "        preds_real = preds_scaled * max_rul\n",
    "        # Calculate true RMSE\n",
    "        mse = mean_squared_error(y_test_real, preds_real)\n",
    "        current_rmse = np.sqrt(mse)\n",
    "        scheduler.step(current_rmse)\n",
    "    \n",
    "    # Save model if it's the best so far\n",
    "    if current_rmse < best_test_rmse:\n",
    "        best_test_rmse = current_rmse\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        print(f\"Epoch {epoch+1}: New Best RMSE: {current_rmse:.2f}\")\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}: Train Loss {avg_train_loss:.6f} | Test RMSE {current_rmse:.2f}\")\n",
    "\n",
    "# 4. Load the best weights back\n",
    "print(f\"Training complete. Best RMSE: {best_test_rmse:.2f}\")\n",
    "model.load_state_dict(best_model_wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f26dc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import EngineRULPredictor\n",
    "\n",
    "X_test_tensor = X_test_tensor.to(\"cpu\")\n",
    "y_test_tensor = y_test_tensor.to(\"cpu\")\n",
    "\n",
    "\n",
    "input_size = 18 # Sensors (21) + Settings (3)\n",
    "model = EngineRULPredictor(input_size=input_size, hidden_size=512, num_layers=2, dropout=0.2)\n",
    "\n",
    "model.load_state_dict(torch.load('models/lstm_model.pth', map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "\n",
    "model.to(\"cpu\")\n",
    "\n",
    "#X_test_tensor = torch.tensor(X_test_seq, dtype=torch.float32).to(device)\n",
    "# y_test_seq is the original TRUE RUL (not scaled)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Predict (Output is 0-1)\n",
    "    preds_scaled = model(X_test_tensor)\n",
    "    # Un-scale (Output becomes 0-300)\n",
    "    preds_real = preds_scaled.cpu().numpy().flatten() * max_rul\n",
    "\n",
    "# Calculate RMSE on real values\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(y_test_seq, preds_real)\n",
    "print(f\"Test RMSE: {np.sqrt(mse):.2f}\")\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(y_test_seq[:400], label='True RUL')\n",
    "plt.plot(preds_real[:400], label='Predicted RUL')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8972e7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import EngineRULPredictor\n",
    "\n",
    "X_test_tensor = X_test_tensor.to(\"cpu\")\n",
    "y_test_tensor = y_test_tensor.to(\"cpu\")\n",
    "\n",
    "print(f\"X_test_tensor.shape: {X_test_tensor.shape}\")\n",
    "print(f\"y_test_tensor.shape: {y_test_tensor.shape}\")\n",
    "\n",
    "print(f\"y_test_tensor[0]: {y_test_tensor[0] * max_rul}\")\n",
    "\n",
    "print(f\"y_test_seq[0]: {y_test_seq[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386a87d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 18 # Sensors (21) + Settings (3)\n",
    "model = EngineRULPredictor(input_size=input_size, hidden_size=512, num_layers=2, dropout=0.2)\n",
    "\n",
    "model.load_state_dict(torch.load('models/lstm_model.pth', map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "\n",
    "model.to(\"cpu\")\n",
    "\n",
    "#X_test_tensor = torch.tensor(X_test_seq, dtype=torch.float32).to(device)\n",
    "# y_test_seq is the original TRUE RUL (not scaled)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Predict (Output is 0-1)\n",
    "    preds_scaled = model(X_test_tensor)\n",
    "    # Un-scale (Output becomes 0-300)\n",
    "    preds_real = preds_scaled.cpu().numpy().flatten() * max_rul\n",
    "\n",
    "# Calculate RMSE on real values\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(y_test_seq, preds_real)\n",
    "print(f\"Test RMSE: {np.sqrt(mse):.2f}\")\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(y_test_seq[:400], label='True RUL')\n",
    "plt.plot(preds_real[:400], label='Predicted RUL')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874f58d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import torch\n",
    "\n",
    "# 1. Create a folder to keep your project clean\n",
    "if not os.path.exists('models'):\n",
    "    os.makedirs('models')\n",
    "\n",
    "# 2. Save the Scaler using joblib\n",
    "joblib.dump(scaler, 'models/scaler.pkl')\n",
    "print(\"Scaler saved to models/scaler.pkl\")\n",
    "\n",
    "# 3. Save the Model using torch\n",
    "torch.save(best_model_wts, 'models/lstm_model.pth')\n",
    "print(\"Model weights saved to models/lstm_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc6854d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class EngineData(BaseModel):\n",
    "    unit_nr: int\n",
    "    time_cycles: int\n",
    "    setting_1: float\n",
    "    setting_2: float\n",
    "    setting_3: float\n",
    "    s_1: float\n",
    "    s_2: float\n",
    "    s_3: float\n",
    "    s_4: float\n",
    "    s_5: float\n",
    "    s_6: float\n",
    "    s_7: float\n",
    "    s_8: float\n",
    "    s_9: float\n",
    "    s_10: float\n",
    "    s_11: float\n",
    "    s_12: float\n",
    "    s_13: float\n",
    "    s_14: float\n",
    "    s_15: float\n",
    "    s_16: float\n",
    "    s_17: float\n",
    "    s_18: float\n",
    "    s_19: float\n",
    "    s_20: float\n",
    "    s_21: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fe5a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick whichever frame you want to validate (raw or scaled)\n",
    "df_for_schema = data_df[col_names]          # or scaled_data_df[col_names]\n",
    "\n",
    "records = df_for_schema.to_dict(orient=\"records\")\n",
    "engine_rows = [EngineData.model_validate(rec) for rec in records]\n",
    "\n",
    "# single row example\n",
    "first_engine = EngineData.model_validate(df_for_schema.iloc[0].to_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142ba374",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_schema.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f11fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(records[0])\n",
    "print(first_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc1506c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"len engine_rows: {len(engine_rows)}\")\n",
    "\n",
    "print(f\"engine_rows[0]: {engine_rows[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414911e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class InferencePayload(BaseModel):\n",
    "    engine_data_sequence: List[EngineData] = Field(min_length=1, max_length=50)\n",
    "\n",
    "    def to_dataframe(self) -> pd.DataFrame:\n",
    "        data_dicts = [edata.model_dump() for edata in self.engine_data_sequence]\n",
    "        return pd.DataFrame(data_dicts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec7033f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import EngineRULPredictor\n",
    "import torch\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "input_size = 18 # Sensors (21) + Settings (3)\n",
    "model = EngineRULPredictor(input_size=input_size, hidden_size=512, num_layers=2, dropout=0.2)\n",
    "\n",
    "model.load_state_dict(torch.load('models/lstm_model.pth', map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "\n",
    "scaler = joblib.load('models/scaler.pkl') # <--- 2. Load the scaler\n",
    "\n",
    "\n",
    "\n",
    "index_names = ['unit_nr', 'time_cycles']\n",
    "setting_names = ['setting_1', 'setting_2', 'setting_3']\n",
    "sensor_names = ['s_{}'.format(i) for i in range(1, 22)] \n",
    "col_names = index_names + setting_names + sensor_names\n",
    "df = pd.read_csv(\"data/train_FD001.txt\", sep=\"\\s+\", header=None, names=col_names)\n",
    "df = df.head(50)  # Use only the first 50 rows for prediction\n",
    "\n",
    "# Convert payload to DataFrame\n",
    "#df = payload.to_dataframe()\n",
    "\n",
    "df = df.drop(columns=[\"unit_nr\", \"time_cycles\"])  # Drop non-feature columns for scaling\n",
    "\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501a083a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#features_to_drop = ['unit_nr', 'time_cycles', \"s_1\", \"s_5\", \"s_10\", \"s_16\", \"s_18\", \"s_19\"]\n",
    "#df = df.drop(columns=features_to_drop)\n",
    "scaled = scaler.transform(df)\n",
    "\n",
    "df = pd.DataFrame(scaled, columns=df.columns, index=df.index)\n",
    "\n",
    "df.head()\n",
    "features_to_drop = [\"s_1\", \"s_5\", \"s_10\", \"s_16\", \"s_18\", \"s_19\"]\n",
    "df = df.drop(columns=features_to_drop)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debfce88",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = torch.tensor(df.to_numpy(), dtype=torch.float32)\n",
    "input_tensor = input_tensor.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "print(f\"input_tensor.shape: {input_tensor.shape}\")\n",
    "\n",
    "# Perform prediction\n",
    "with torch.no_grad():\n",
    "    prediction = model(input_tensor)\n",
    "    preds_real = prediction.cpu().numpy().flatten() * max_rul\n",
    "\n",
    "    print(f\"Predicted RUL (real scale): {preds_real}\")\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10,6))\n",
    "#plt.plot(y_test_seq[:400], label='True RUL')\n",
    "plt.plot(preds_real, label='Predicted RUL')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Assuming the model outputs a single RUL value\n",
    "rul_prediction = prediction.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1609af43",
   "metadata": {},
   "outputs": [],
   "source": [
    "rul_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6df231",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(\"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "scaler = joblib.load(\"models/scaler.pkl\")\n",
    "max_rul = 542  # use the same value you trained with\n",
    "\n",
    "df = pd.read_csv(\"data/train_FD001.txt\", sep=r\"\\s+\", header=None, names=col_names).head(50)\n",
    "df_no_id = df.drop(columns=[\"unit_nr\", \"time_cycles\"])\n",
    "df_scaled = pd.DataFrame(scaler.transform(df_no_id), columns=df_no_id.columns, index=df.index)\n",
    "df_model = df_scaled.drop(columns=[\"s_1\",\"s_5\",\"s_10\",\"s_16\",\"s_18\",\"s_19\"])\n",
    "\n",
    "input_tensor = torch.tensor(df_model.to_numpy(), dtype=torch.float32).unsqueeze(0)\n",
    "with torch.no_grad():\n",
    "    rul_scaled = model(input_tensor).item()\n",
    "rul_cycles = rul_scaled * max_rul\n",
    "\n",
    "print(f\"Predicted RUL (cycles): {rul_cycles:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0840f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import EngineRULPredictor\n",
    "\n",
    "X_test_tensor = X_test_tensor.to(\"cpu\")\n",
    "y_test_tensor = y_test_tensor.to(\"cpu\")\n",
    "\n",
    "print(f\"X_test_tensor.shape: {X_test_tensor.shape}\")\n",
    "\n",
    "input_size = 18 # Sensors (21) + Settings (3)\n",
    "model = EngineRULPredictor(input_size=input_size, hidden_size=512, num_layers=2, dropout=0.2)\n",
    "\n",
    "model.load_state_dict(torch.load('models/lstm_model.pth', map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "\n",
    "model.to(\"cpu\")\n",
    "\n",
    "#X_test_tensor = torch.tensor(X_test_seq, dtype=torch.float32).to(device)\n",
    "# y_test_seq is the original TRUE RUL (not scaled)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Predict (Output is 0-1)\n",
    "    preds_scaled = model(X_test_tensor)\n",
    "    # Un-scale (Output becomes 0-300)\n",
    "    preds_real = preds_scaled.cpu().numpy().flatten() * max_rul\n",
    "\n",
    "# Calculate RMSE on real values\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(y_test_seq, preds_real)\n",
    "print(f\"Test RMSE: {np.sqrt(mse):.2f}\")\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(y_test_seq[:400], label='True RUL')\n",
    "plt.plot(preds_real[:400], label='Predicted RUL')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09e63a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:4: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:4: SyntaxWarning: invalid escape sequence '\\s'\n",
      "/tmp/ipykernel_2591124/3633571523.py:4: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  rul_df = pd.read_csv(\"data/RUL_FD001.txt\", sep=\"\\s+\", header=None, names=[\"rul\"])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rul</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unit_nr</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         rul\n",
       "unit_nr     \n",
       "2         98"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd    \n",
    "unit_nr = 2\n",
    "\n",
    "rul_df = pd.read_csv(\"data/RUL_FD001.txt\", sep=\"\\s+\", header=None, names=[\"rul\"])\n",
    "rul_df.index = range(1, len(rul_df) + 1)\n",
    "rul_df.index.name = \"unit_nr\"\n",
    "rul_df = rul_df[rul_df.index == unit_nr]\n",
    "\n",
    "rul_df.head()\n",
    "#rul_series = rul_df[\"rul\"]\n",
    "#rul_series.head() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9ae9e7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rul_df.values.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371c9843",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gemini-lstm-next-frame-prediction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
